<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>research | Jaspreet  Ranjit</title>
    <meta name="author" content="Jaspreet  Ranjit">
    <meta name="description" content="Brief Descriptions of my current and past projects">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
    <!-- <link rel="stylesheet" href="/assets/css/mdb.min.css?62a43d1430ddb46fc4886f9d0e3b49b8"> -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/emoji.001.png?55458f353d5f81229f0ebee03f10e7f6">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="https://jr4fs.github.io/research/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script>
    <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jaspreet </span>Ranjit</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/research/">research<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications &amp; articles</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/internships/">internships</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/service/">service &amp; projects</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
<div class="post">

  <header class="post-header">
    <h1 class="post-title">research</h1>
    <p class="post-description">Brief Descriptions of my current and past projects</p>
  </header>

  <article>
    <h3 id="csci-698---how-to-evaluate-human-ai-collaborative-systems"><strong>CSCI 698 - How to Evaluate Human-AI Collaborative Systems</strong></h3>
<p>In this talk, we explore how to rigorously evaluate systems where humans and AI work together. We’ll compare intrinsic and extrinsic evaluations, discuss what each reveals, and highlight why human-centered metrics are essential for understanding real-world effectiveness.</p>

<!-- Embedded video (example: YouTube) -->
<iframe width="660" height="415" src="https://www.youtube.com/embed/JY2CS-BNK-w?si=xvDTYlPxFh17a0P4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<!-- <embed src="/_assets/slides.pdf" type="application/pdf" width="100%" height="600px" title="Embedded PDF Viewer" /> -->
<h4 id="slides">*Slides**</h4>

<embed src="/assets/slides.pdf" type="application/pdf" width="100%" height="600px" title="Embedded PDF Viewer"></embed>

<p><a name="toc"></a> I am grateful to be advised by <a href="https://swabhs.com/" rel="external nofollow noopener" target="_blank">Prof. Swabha Swayamdipta</a> and to be collaborating with students in the USC NLP Department and USC Dworak-Peck School of Social Work!</p>

<h3 id="high-level-motivation"><strong>High Level Motivation</strong></h3>
<p>My research focuses on human-centered AI alignment, with an emphasis on how AI systems can adapt to domain-specific tasks in high-impact societal applications. I study how domain experts and AI can be effective partners in knowledge extraction and, more broadly, in applications for social good. To this end, I develop new methods and human-grounded evaluations designed to enable outcomes with real societal impact in social services, and healthcare by collaborating closely with community partners and stakeholders.</p>

<h3 id="past-work-and-projects-">
<strong>Past Work and Projects</strong> <a name="current"></a>
</h3>

<h4 id="uncovering-intervention-opportunities-for-suicide-prevention-with-language-model-assistants-">Uncovering Intervention Opportunities for Suicide Prevention with Language Model Assistants <a name="current-first"></a>
</h4>

<p>The National Violent Death Reporting System (NVDRS) documents information about suicides in the United States, including free text narratives (e.g., circumstances surrounding a suicide). In a demanding public health data pipeline, annotators manually extract structured information from death investigation records following extensive guidelines developed painstakingly by experts. In this work, we facilitate data-driven insights from the NVDRS data to support the development of novel suicide interventions by investigating the value of language models (LMs) as efficient assistants to these (a) data annotators and (b) experts. We find that LM predictions match existing data annotations about 85% of the time across 50 NVDRS variables. In the cases where the LM disagrees with existing annotations, expert review reveals that LM assistants can surface annotation discrepancies 38% of the time. Finally, we introduce a human-in-the-loop algorithm to assist experts in efficiently building and refining guidelines for annotating new variables by allowing them to focus only on providing feedback for incorrect LM predictions. We apply our algorithm to a real-world case study for a new variable that characterizes victim interactions with lawyers and demonstrate that it achieves comparable annotation quality with a laborious manual approach. Our findings provide evidence that LMs can serve as effective assistants to public health researchers who handle sensitive data in high-stakes scenarios.</p>
<div class="publications">
  <h2 class="bibliography">2025</h2>
<ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 preview">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/nvdrs-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/nvdrs-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/nvdrs-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/nvdrs.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="nvdrs.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>
</div>
<!-- Entry bib key -->
        <div id="ranjit2025interventions" class="col-sm-8">
        <!-- Title -->
        <div class="title">Uncovering Intervention Opportunities for Suicide Prevention with Language Model Assistants</div>
        <!-- Author -->
        <div class="author">
        

        <em>Jaspreet Ranjit</em>, Hyundong J. Cho, Claire J. Smerdon, and
          <span class="more-authors" title="click to view 5 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '5 more authors' ? 'Yoonsoo Nam, Myles Phung, Jonathan May, John R. Blosnich, Swabha Swayamdipta' : '5 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">5 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In EAAMO’25, GenAI4Health NeurIPS’25</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            
            <a class="award btn btn-sm z-depth-1 rounded" role="button">Runner up for best doctoral oral presentation @ ShowCAIS’25</a>
          
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://github.com/dill-lab/interventions_lm_assistants" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          
          
          
          
          <!-- Hidden Award block -->
          <div class="award hidden d-print-inline">
            <p></p>
<p>Runner up for best doctoral oral presentatino at ShowCAIS 2025</p>

          </div>
        
        
          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The National Violent Death Reporting System (NVDRS) documents in-
formation about suicides in the United States, including free text narra-
tives (e.g., circumstances surrounding a suicide). In a demanding public
health data pipeline, annotators manually extract structured information
from death investigation records following extensive guidelines developed
painstakingly by experts. In this work, we facilitate data-driven insights
from the NVDRS data to support the development of novel suicide inter-
ventions by investigating the value of language models (LMs) as efficient
assistants to these (a) data annotators and (b) experts. We find that LM
predictions match existing data annotations about 85% of the time across
50 NVDRS variables. In the cases where the LM disagrees with existing an-
notations, expert review reveals that LM assistants can surface annotation
discrepancies 38% of the time. Finally, we introduce a human-in-the-loop
algorithm to assist experts in efficiently building and refining guidelines for
annotating new variables by allowing them to focus only on providing feed-
back for incorrect LM predictions. We apply our algorithm to a real-world
case study for a new variable that characterizes victim interactions with
lawyers and demonstrate that it achieves comparable annotation quality
with a laborious manual approach. Our findings provide evidence that LMs
can serve as effective assistants to public health researchers who handle
sensitive data in high-stakes scenarios.</p>
          </div>
        </div>
      </div>
</li></ol>
</div>

<h4 id="oath-frames-characterizing-online-attitudes-towards-homelessness-via-llm-assistants-">OATH-Frames: Characterizing Online Attitudes Towards Homelessness via LLM Assistants <a name="current-first"></a>
</h4>

<p>Public attitudes towards key societal issues, expressed on online media, are of immense value in policy and reform efforts, yet challenging to understand at scale. We study one such social issue: homelessness in the U.S., by leveraging the remarkable capabilities of large language models to assist social work experts in analyzing millions of posts from Twitter. We introduce a framing typology: Online Attitudes Towards Homelessness (OATH) Frames: nine hierarchical frames capturing critiques, responses and perceptions. We release annotations with varying degrees of assistance from language models, with immense benefits in scaling: 6.5x speedup in annotation time while only incurring a 3 point F1 reduction in performance with respect to the domain experts. Our experiments demonstrate the value of modeling OATH-Frames over existing sentiment and toxicity classifiers. Our large-scale analysis with predicted OATH-Frames on 2.4M posts on homelessness reveal key trends in attitudes across states, time periods and vulnerable populations, enabling new insights on the issue. Our work provides a general framework to understand nuanced public attitudes at scale, on issues beyond homelessness.</p>

<p>In this project, we aimed to:</p>

<ol>
  <li>Evaluate the effectiveness of language models at reasoning about attitudes towards homelessness by developing a codebook grounded in framing theory from sociology (Goffman, 1974)</li>
  <li>Finetune models on an expert annotated dataset to infer our frames developed from our codebook</li>
  <li>Conduct analyses with respect to socio-political dimensions to characterize how attitudes differ across regionality.</li>
</ol>

<div class="publications">
  <h2 class="bibliography">2024</h2>
<ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 preview">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/oath-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/oath-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/oath-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/oath.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="oath.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>
</div>
<!-- Entry bib key -->
        <div id="ranjit2024oath" class="col-sm-8">
        <!-- Title -->
        <div class="title">OATH-Frames: Characterizing Online Attitudes Towards Homelessness via LLM Assistants</div>
        <!-- Author -->
        <div class="author">
        

        <em>Jaspreet Ranjit</em>, Brihi Joshi, Rebecca Dorn, and
          <span class="more-authors" title="click to view 6 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '6 more authors' ? 'Laura Petry, Olga Koumoundouros, Jayne Bottarini, Peichen Liu, Eric Rice, Swabha Swayamdipta' : '6 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">6 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of EMNLP</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            
            <a class="award btn btn-sm z-depth-1 rounded" role="button">Outstanding Paper Award @ EMNLP 2024; Best Poster @ ShowCAIS’24</a>
          
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://dill-lab.github.io/oath-frames/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a>
            <a href="https://github.com/dill-lab/oath-frames" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          
          
          
          
          <!-- Hidden Award block -->
          <div class="award hidden d-print-inline">
            <p></p>
<p>Outstanding Paper Award @ EMNLP 2024; Jaspreet received a <a href="https://sites.google.com/usc.edu/showcais-2024/awards?authuser=0" rel="external nofollow noopener" target="_blank">best poster award</a> at USC CAIS’s annual symposium, <a href="https://sites.google.com/usc.edu/showcais-2024/" rel="external nofollow noopener" target="_blank">ShowCAIS</a> in Spring 2024.</p>

          </div>
        
        
          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Homelessness in the U.S. is widespread; individual beliefs and attitudes towards homelessness—often expressed on social media are complex and nuanced (e.g. critical as well as sympathetic). Such attitudes can be challenging to summarize at scale, obfuscating the broader public opinion which advocacy organizations use to guide public policy and reform efforts. Our work proposes an approach to enable a large-scale study on homelessness via two major contributions. First, with the help of domain experts in social work and their trainees, we characterize Online Attitudes towards Homelessness in nine hierarchical frames (OATH-Frames) on a collection of 4K social media posts. Further, in an effort to ease the annotation of these frames, we employ GPT-4 as an LLM assistant to the experts; GPT-4 + Expert annotation presents an attractive trade off owing to a 6.5× speedup in annotation time despite only incurring a 2 point F1 difference in annotation performance. Our effort results in a collection of 8K social media posts labeled by domain and trained experts (with and without GPT-4 assistance). Second, using predicted OATH-Frames on a Flan-T5-Large model trained on our data, we perform a large-scale analysis on 2.4M posts on homelessness. We find that posts that contain mentions of west coast states express more harmful generalizations of people experiencing homelessness (PEH) compared to posts about east coast states. We also find marked differences in attitudes across vulnerable populations as they are compared to PEH as being either more or less deserving of aid.</p>
          </div>
        </div>
      </div>
</li></ol>
</div>

<h4 id="variation-of-gender-biases-in-visual-recognition-models-before-and-after-finetuning-">Variation of Gender Biases in Visual Recognition Models Before and After Finetuning <a name="current-second"></a>
</h4>

<p>In collaboration with <a href="https://tianlu-wang.github.io/" rel="external nofollow noopener" target="_blank">Tianlu Wang</a>, <a href="https://www.rayb.info/" rel="external nofollow noopener" target="_blank">Baishakhi Ray</a>, and <a href="https://www.cs.rice.edu/~vo9/" rel="external nofollow noopener" target="_blank">Vicente Ordonez</a>, we introduce a framework to measure how biases change before and after fine-tuning a large scale visual recognition model for a downstream task.</p>

<p>Many computer vision systems today rely on models typically pretrained on large scale datasets. While bias mitigation techniques have been developed for tuning models for downstream tasks, it is currently unclear what are the effects of biases already encoded in a pretrained model. Our framework incorporates sets of canonical images representing individual and pairs of concepts to highlight changes in biases for an array of off-the-shelf pretrained models across model sizes, dataset sizes, and training objectives.</p>

<p>Through our analyses, we find that:</p>
<ol>
  <li>Supervised models trained on datasets such as ImageNet-21k are more likely to retain their pretraining biases regardless of the target dataset compared to self-supervised models.</li>
  <li>Models finetuned on larger scale datasets are more likely to introduce new biased associations. Our results also suggest that</li>
  <li>Biases can transfer to finetuned models and the finetuning objective and dataset can impact the extent of transferred biases.</li>
</ol>

<p><a href="https://arxiv.org/abs/2303.07615" rel="external nofollow noopener" target="_blank">Our work</a> was recently accepted at the <a href="https://www.afciworkshop.org/" rel="external nofollow noopener" target="_blank">Workshop on Algorithmic Fairness through the Lens of Time</a> at NeuRIPS 2023. New Orleans, LA.</p>

<div class="publications">
  <h2 class="bibliography">2023</h2>
<ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 preview">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/variation_gender_biases-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/variation_gender_biases-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/variation_gender_biases-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/variation_gender_biases.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="variation_gender_biases.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>
</div>
<!-- Entry bib key -->
        <div id="ranjit2023variation" class="col-sm-8">
        <!-- Title -->
        <div class="title">Variation of Gender Biases in Visual Recognition Models Before and After Finetuning</div>
        <!-- Author -->
        <div class="author">
        

        <em>Jaspreet Ranjit</em>, Tianlu Wang, Baishakhi Ray, and
          <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Vicente Ordonez' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Workshop on Algorithmic Fairness through the Lens of Time at NeuRIPS</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/abs/2303.07615" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          
          
        
          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We introduce a framework to measure how biases change before and after fine-tuning a large scale visual recognition model for a downstream task. Deep learning models trained on increasing amounts of data are known to encode societal biases. Many computer vision systems today rely on models typically pretrained on large scale datasets. While bias mitigation techniques have been developed for tuning models for downstream tasks, it is currently unclear what are the effects of biases already encoded in a pretrained model. Our framework incorporates sets of canonical images representing individual and pairs of concepts to highlight changes in biases for an array of off-the-shelf pretrained models across model sizes, dataset sizes, and training objectives. Through our analyses, we find that (1) supervised models trained on datasets such as ImageNet-21k are more likely to retain their pretraining biases regardless of the target dataset compared to self-supervised models. We also find that (2) models finetuned on larger scale datasets are more likely to introduce new biased associations. Our results also suggest that (3) biases can transfer to finetuned models and the finetuning objective and dataset can impact the extent of transferred biases.</p>
          </div>
        </div>
      </div>
</li></ol>
</div>

<h4 id="scenario2vector-scenario-description-language-based-embeddings-for-traffic-situations-">Scenario2Vector: scenario description language based embeddings for traffic situations <a name="current-second"></a>
</h4>

<p>In collaboration with <a href="https://www.linkedin.com/in/aron-harder-a81052115/" rel="external nofollow noopener" target="_blank">Aron Harder</a> and <a href="https://www.madhurbehl.com/" rel="external nofollow noopener" target="_blank">Madhur Behl</a>, we propose Scenario2Vector - a Scenario Description Language (SDL) based embedding for traffic situations that allows us to automatically search for similar traffic situations from large AV data-sets. Our SDL embedding distills a traffic situation experienced by an AV into its canonical components - actors, actions, and the traffic scene. We can then use this embedding to evaluate similarity of different traffic situations in vector space.</p>

<p>Safety assessments for automated vehicles need to evolve beyond the existing voluntary self-reporting. There is no comprehensive measuring stick that can compare how far each AV developer is in terms of safety. Our goal in this research is to answer the following question: How can we fairly compare two different AV implementations? In doing so, the aim of this work is to make progress towards an innovative certification method allowing for a fair comparison between AVs by comparing them on similar traffic situations.</p>

<p>The goal of our research is to provide a common metric that will facilitate the comparison of different autonomous vehicle algorithms. In order to compare the different AVs, we need to observe them under similar traffic conditions or scenarios. Our goal therefore is to find similar traffic scenarios from the datasets generated by different AVs. Having found similar traffic situations, we can then observe if the output of one AV is more safe/optimal compared to another. To this end, we also present a first of its kind -Traffic Scenario Similarity (TSS) dataset. This dataset contains 100 traffic video samples (scenarios) and for each sample, it contains 6 candidate scenario videos ranked by human participants based on its similarity to the baseline sample.</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3450267.3450544" rel="external nofollow noopener" target="_blank">Our work</a> was accepted in the <a href="http://iccps.acm.org/2021/" rel="external nofollow noopener" target="_blank">Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems</a> at ICCPS 2021. Nashville, TN.</p>

<div class="publications">
  <h2 class="bibliography">2021</h2>
<ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
<div class="col-sm-2 preview">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/sce2vec-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/sce2vec-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/sce2vec-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/sce2vec.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="sce2vec.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

</figure>
</div>
<!-- Entry bib key -->
        <div id="sce2vec" class="col-sm-8">
        <!-- Title -->
        <div class="title">Scenario2Vector: Scenario Description Language Based Embeddings for Traffic Situations</div>
        <!-- Author -->
        <div class="author">
        

        Aron Harder, <em>Jaspreet Ranjit</em>, and Madhur Behl</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems</em>, 2021
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://doi.org/10.1145/3450267.3450544" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
          </div>
          
          
          
          
          
        
          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>A popular metric for measuring progress in autonomous driving has been the "miles per intervention". This is nowhere near a sufficient metric and it does not allow for a fair comparison between the capabilities of two autonomous vehicles (AVs). In this paper we propose Scenario2Vector - a Scenario Description Language (SDL) based embedding for traffic situations that allows us to automatically search for similar traffic situations from large AV data-sets. Our SDL embedding distills a traffic situation experienced by an AV into its canonical components - actors, actions, and the traffic scene. We can then use this embedding to evaluate similarity of different traffic situations in vector space. We have also created a first of its kind, Traffic Scenario Similarity (TSS) dataset which contains human ranking annotations for the similarity between traffic scenarios. Using the TSS data, we compare our SDL embedding -with textual caption based search methods such as Sentence2Vector. We find that Scenario2Vector outperforms Sentence2Vector by 13% ; and is a promising step towards enabling fair comparisons among AVs by inspecting how they perform in similar traffic situations. We hope that Scenario2Vector can have a similar impact to the AV community that Word2Vec/Sent2Vec have had in Natural Language Processing datasets.</p>
          </div>
        </div>
      </div>
</li></ol>
</div>

  </article>
</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Jaspreet  Ranjit. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="/assets/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="/assets/js/mdb.min.js"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script>
  <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script>
  <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

    
  </body>
</html>
